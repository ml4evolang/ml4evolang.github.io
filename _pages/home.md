---
layout: project
urltitle:  "Machine Learning and the Evolution of Language (ML4EvoLang)"
title: "Machine Learning and the Evolution of Language (ML4EvoLang)"
categories: workshop, emergent communication, language evolution, natural language, machine learning, computational modeling, ml4evolang, evolang, jcole, 2022
permalink: /
bibtex: true
paper: true
acknowledgements: ""
---

<br />
<div class="row">
  <div class="col-xs-12">
    <center><h1>Machine Learning and the Evolution of Language (ml4evolang)</h1></center>
    <center><h2>Sept, 2022. JCoLE Workshop, Kanazawa, Japan and Online.</h2></center>
    <!-- TODO: add exact date here -->
    <center>Co-located with <a href="https://sites.google.com/view/joint-conf-language-evolution/home" target="_blank">JCoLE, September 5th - 8th, 2022</a></center>
    <!--center>Ask the panelists! Submit your questions at <a href="https://app.sli.do/event/zf5ggt9a/live/questions" target="_blank">sli.do</a>.</center-->
  </div>
</div>

<br />

<div class="row">
    <div class="col-xs-12">
        <p>
        The goal of this workshop is to build a bridge between the language evolution community and the machine learning community. In the past three decades, numerous studies have attempted to mimic the evolution of language with human participants and agent-based computational models. Meanwhile, in the last decade,  the machine learning community has similarly made strides in simulating emergent communication with deep and reinforcement learning methods. Although both areas of research have similar interests and work on similar questions, there has been little crosstalk between them so far.
        This is unfortunate, since the progress in machine learning and other areas of AI may allow language evolution researchers to model phenomena that they could not model before.
        At the same time, theoretical and experimental knowledge of language evolution coming from the linguistics community may help focus models of emergent communication used by the machine learning community. The goal of this workshop is to relate these two areas by bringing together researchers from both backgrounds, establishing common ground, bootstrapping a mutual dialogue between them, and discussing the potential pitfalls of incorporating machine learning methods in the study of language evolution.
        </p>
    </div>
</div>

<br />

<div class="row" id="schedule">
  <div class="col-md-4 col-xs-12">
    <h2>Preliminary Schedule</h2>
  </div>
  <div class="col-md-8 col-xs-12">
      <select id="timezone-select" class="form-control"></select>
  </div>
</div>
<div class="row">
</div>
<div class="row">
  <div class="col-xs-12">
    <table class="table table-striped" id="schedule-table">
    <tbody>
    <!-- <tr> <th scope="row" data-time="19:00">t.b.d. </th> <td>Welcome and quick intro (Limor Raviv)</td></tr> -->
    <!-- <tr> <th scope="row" data-time="19:10">t.b.d. + 10</th> <td>Intro to machine learning methods (Florian Strub)</td></tr> -->
    <!-- <tr> <th scope="row" data-time="19:40">t.b.d. + 40</th> <td>Invited Talk 1: Matt Spike</td></tr> -->
    <!-- <tr> <th scope="row" data-time="20:00">t.b.d. + 60</th> <td>Invited Talk 2: Katie Mudd</td></tr> -->
    <!-- <tr> <th scope="row" data-time="20:20">t.b.d. + 80</th> <td>Poster session</td></tr> -->
    <!-- <tr> <th scope="row" data-time="20:40">t.b.d. + 100</th> <td>Break</td></tr> -->
    <!-- <tr> <th scope="row" data-time="21:00">t.b.d. + 120</th> <td>Invited Talk 3: Rahma Chaabouni</td></tr> -->
    <!-- <tr> <th scope="row" data-time="21:25">t.b.d. + 145</th> <td>Invited Talk 4: Douwe Kiela</td></tr> -->
    <!-- <tr> <th scope="row" data-time="21:50">t.b.d. + 170</th> <td>Lessons from the past (Bart de Boer)</td></tr> -->
    <!-- <tr> <th scope="row" data-time="22:05">t.b.d. + 185</th> <td>Identifying some gaps (Lukas Galke)</td></tr> -->
    <!-- <tr> <th scope="row" data-time="22:20">t.b.d. + 200</th> <td>Panel discussion (Chair: Limor Raviv)</td></tr> -->
    <!-- <tr> <th scope="row" data-time="22:55">t.b.d. + 215</th> <td>Final remarks (Mathieu Rita)</td></tr> -->
    <tr> <th scope="row">t.b.d. </th> <td>Welcome and quick intro (Limor Raviv)</td></tr>
    <tr> <th scope="row">t.b.d. + 10</th> <td>Intro to machine learning methods (Florian Strub)</td></tr>
    <tr> <th scope="row">t.b.d. + 40</th> <td>Invited Talk 1: Matt Spike</td></tr>
    <tr> <th scope="row">t.b.d. + 60</th> <td>Invited Talk 2: Katie Mudd</td></tr>
    <tr> <th scope="row">t.b.d. + 80</th> <td>Poster session</td></tr>
    <tr> <th scope="row">t.b.d. + 100</th> <td>Break</td></tr>
    <tr> <th scope="row">t.b.d. + 120</th> <td>Invited Talk 3: Rahma Chaabouni</td></tr>
    <tr> <th scope="row">t.b.d. + 145</th> <td>Invited Talk 4: Douwe Kiela</td></tr>
    <tr> <th scope="row">t.b.d. + 170</th> <td>Lessons from the past (Bart de Boer)</td></tr>
    <tr> <th scope="row">t.b.d. + 185</th> <td>Identifying some gaps (Lukas Galke)</td></tr>
    <tr> <th scope="row">t.b.d. + 200</th> <td>Panel discussion (Chair: Limor Raviv)</td></tr>
    <tr> <th scope="row">t.b.d. + 215</th> <td>Final remarks (Mathieu Rita)</td></tr>


    <!-- <tr> <th scope="row" data-time="09:00">09:00 AM</th> <td> -->
    <!--   Talk 1: Roger Levy<br />Semantics, Pragmatics, and Context in Human Grounded Language Understanding -->
    <!--   <a data-toggle="collapse" href="#schedule-talk1" aria-cexpanded="false" aria-controls="schedule-talk1">[Abstract]</a> -->
    <!--   <a target="_blank" href="/static/slides-2021/roger_levy.pdf">[Slides]</a> -->
    <!--   <div class="collapse" id="schedule-talk1"> -->
    <!--     Abstract: Computational systems for grounded language understanding have seen impressive advances over the last decade, due largely to advances in multimodal datasets, neural and symbolic modeling techniques, and computational power. But human meaning interpretation in grounded contexts remains far deeper and more sophisticated. In this talk I describe several recent studies in our research group that illustrate the subtlety and richness of human meaning interpretation using very simple, experimentally controlled utterances and visual grounding contexts. These studies shed light on the compositional structure of the semantic representations underlying human language comprehension, their relationship with the pragmatic inference mechanisms that support contextually conditioned interpretation, and the likely requirements for truly human-like language understanding in artificial systems. -->
    <!--   </div> -->
    <!-- </td></tr> -->
    <!-- <tr> <th scope="row" data-time="09:45">09:45 AM</th> <td> -->
    <!--   Talk 2: Stefanie Tellex<br />Towards Complex Language in Partially Observed Environments -->
    <!--   <a data-toggle="collapse" href="#schedule-talk2" aria-cexpanded="false" aria-controls="schedule-talk2">[Abstract]</a> -->
    <!--   <div class="collapse" id="schedule-talk2"> -->
    <!--     Abstract: Robots can act as a force multiplier for people, whether a robot assisting an astronaut with a repair on the International Space station, a UAV taking flight over our cities, or an autonomous vehicle driving through our streets. Existing approaches use action-based representations that do not capture the goal-based meaning of a language expression and do not generalize to partially observed environments.  The aim of my research program is to create autonomous robots that can understand complex goal-based commands and execute those commands in partially observed, dynamic environments.  I will describe demonstrations of object-search in a POMDP setting with information about object locations provided by language, and mapping between English and Linear Temporal Logic, enabling a robot to understand complex natural language commands in city-scale environments.  These advances represent steps towards robots that interpret complex natural language commands in partially observed environments using a decision theoretic framework. -->
    <!--   </div> -->
    <!-- </td></tr> -->
    <!-- <tr> <th scope="row" data-time="10:30">10:30 AM</th> <td>Break 1</td></tr> -->
    <!-- <tr> <th scope="row" data-time="11:00">11:00 AM</th> <td> -->
    <!--   Talk 3: Katerina Fragkiadaki<br />Linking Language with World Common Sense Using 3D Visual Feature Representations -->
    <!--   <a data-toggle="collapse" href="#schedule-talk3" aria-cexpanded="false" aria-controls="schedule-talk3">[Abstract]</a> -->
    <!--   <div class="collapse" id="schedule-talk3"> -->
    <!--     Abstract: To link language processing with spatial reasoning, we propose associating natural language utterances to a mental workspace of their meaning, encoded as 3-dimensional visual feature representations of the world scenes they describe. We learn such 3-dimensional visual representations---we call them visual imaginations--- by predicting images a mobile agent sees while moving around in the 3D world. The input image streams the agent collects are unprojected into egomotion-stable 3D scene feature maps of the scene, and projected from novel viewpoints to match the observed RGB image views in an end-to-end differentiable manner. We then train modular neural models to generate such 3D feature representations given language utterances, to localize the objects an utterance mentions in the 3D feature representation inferred from an image, and to predict the desired 3D object locations given a manipulation instruction. We empirically show the proposed models outperform by a large margin existing 2D models in spatial reasoning, referential object detection and instruction following, and generalize better across camera viewpoints and object arrangements. -->
    <!--   </div> -->
    <!-- </td> </tr> -->
    <!-- <tr> <th scope="row" data-time="11:45">11:45 AM</th> <td> -->
    <!--   Talk 4: Max Garagnani<br />Action-Perception Circuits for Word Learning and Semantic Grounding -->
    <!--   <a data-toggle="collapse" href="#schedule-talk4" aria-cexpanded="false" aria-controls="schedule-talk4">[Abstract]</a> -->
    <!--   <div class="collapse" id="schedule-talk4"> -->
    <!--     Abstract: Embodied semantic theories posit that word meaning is grounded in the perception and action systems of the human brain. Such theories are supported by a growing body of experimental results, indicating that processing of words belonging to specific semantic categories (e.g., visual object or motor action related, e.g. “sun” or “run”) leads to selective activation of corresponding modality-preferential areas.<br /> -->
    <!--     I highlight a deep, spiking neurocomputational architecture of the left-hemispheric fronto-temporal areas that has been used to simulate and explain putative brain processes underlying word learning and semantic grounding in action and perception. The model closely replicates neuroanatomical and neurobiological features of the relevant brain regions and implements exclusively mechanisms mimicking known cellular- and synaptic-level features of the mammalian cortex. Lastly I discuss some recent experimental evidence confirming the model’s main predictions and conclude by suggesting elements of a unifying theory for the emergence of cognition based on the spontaneous formation of cortically distributed action-perception circuits (APCs) in the brain. -->
    <!--   </div> -->
    <!-- </td></tr> -->
    <!-- <tr> <th scope="row" data-time="12:30">12:30 PM</th> <td>Break 2</td></tr> -->
    <!-- <tr> <th scope="row" data-time="13:00">13:00 PM</th> <td>Panel Discussion</td> </tr> -->
    <!-- <tr> <th scope="row" data-time="14:00">14:00 PM</th> <td>Break 3</td> </tr> -->
    <!-- <tr> <th scope="row" data-time="14:30">14:30 PM</th> <td> -->
    <!--   Talk 5: Yejin Choi<br />Grounded Causal Commonsense Reasoning -->
    <!--   <a data-toggle="collapse" href="#schedule-talk5" aria-cexpanded="false" aria-controls="schedule-talk5">[Abstract]</a> -->
    <!--   <a target="_blank" href="/static/slides-2021/yejin_choi.pdf">[Slides]</a> -->
    <!--   <div class="collapse" id="schedule-talk5"> -->
    <!--     Abstract: In this talk, we will consider Harnad’s symbol grounding problem from three different angles: learning the functional meaning of objects and actions through interactions in a 3D environment, learning the grounded meaning of more complex language by watching YouTube videos at extreme scale, and learning causal commonsense inferences of the visual scenes through a large-scale symbolic knowledge graph. -->
    <!--   </div> -->
    <!-- </td> </tr> -->
    <!-- <tr> <th scope="row" data-time="15:15">15:15 PM</th> <td> -->
    <!--   Talk 6: Justin Johnson<br />Learning Visual Representations from Language -->
    <!--   <a data-toggle="collapse" href="#schedule-6" aria-cexpanded="false" aria-controls="schedule-6">[Abstract]</a> -->
    <!--   <div class="collapse" id="schedule-6"> -->
    <!--     Abstract: Standard practice in vision+language is to treat multimodal vision+language tasks as downstream from vision: generic unimodal representations are combined for multimodal end tasks. In this talk I'll argue that this should be flipped: multimodal vision+language tasks should be used to learn powerful representations that can be transferred to downstream visual representation tasks. Our approach, termed VirTex, uses image captioning as a pretext task for learning visual features. When trained on COCO captions, VirTex learns representations that match or exceed supervised ImageNet pretraining on many downstream visual recognition tasks. I will also discuss our efforts to scale up this algorithm, for which we've created a new dataset of 11.7M high-quality images and natural-language captions. -->
    <!--   </div> -->
    <!-- </td> </tr> -->
    <!-- <tr> <th scope="row" data-time="16:00">16:00 PM</th> <td>Spotlight Presentations</td> </tr> -->
    <!-- <tr> <th scope="row" data-time="16:10">16:10 PM</th> <td>Poster</td> </tr> -->
    <!-- <tr> <th scope="row" data-time="17:30">17:30 PM</th> <td>Break</td> </tr> -->
    <!-- <tr> <th scope="row" data-time="18:00">18:00 PM</th> <td> -->
    <!--   Talk 7: Trevor Darrell (presented by Daniel Fried and Rudy Corona)<br />Modularity in Grounded Interaction -->
    <!--   <a data-toggle="collapse" href="#schedule-talk7" aria-cexpanded="false" aria-controls="schedule-talk7">[Abstract]</a> -->
    <!--   <a target="_blank" href="/static/slides-2021/fried_corona.pdf">[Slides]</a> -->
    <!--   <div class="collapse" id="schedule-talk7"> -->
    <!--     Abstract: Neural networks have made great strides in language grounding, but still leave room for improvement in robustness, ease of design, and interpretability. Modularity, a staple of complex system design, has the potential to help on all of these. We find that modular neural nets outperform their non-modular counterparts on a grounded collaborative dialogue task and in compositional generalization settings for embodied instruction following. -->
    <!--   </div> -->
    <!-- </td> </tr> -->
    <!-- <tr> <th scope="row" data-time="18:45">18:45 PM</th> <td> -->
    <!--   Talk 8: Sandra Waxman<br />How (and how early) do infants link language and cognition? -->
    <!--   <a data-toggle="collapse" href="#schedule-talk8" aria-cexpanded="false" aria-controls="schedule-talk8">[Abstract]</a> -->
    <!--   <div class="collapse" id="schedule-talk8"> -->
    <!--     Abstract: Language is a signature of our species. It is the pathway through which we share the contents of our minds, imagine new ideas and ignite them in others. But how, and how early, do infants link language and thought?  How do they identify which signals are part of their language and discover how these are linked to fundamental representations of objects and events? Infants begin to forge this language-cognition interface in the first months of life. Even before they say their first words, listening to human language promotes core cognitive capacities, including object categorization and rule-learning. Moreover, this precocious link emerges from a broader template that initially includes vocalizations of non-human primates, but is rapidly tuned specifically to human vocalizations. I’ll describe an exquisitely timed developmental cascade, fueled by both ‘nature’ and ‘nurture’, leading infants to discover increasingly precise links between language and cognition, and use this link to learn about their world. -->
    <!--   </div> -->
    <!-- </td> </tr> -->
    <!-- <tr> <th scope="row" data-time="19:30">19:30 PM</th> <td>Closing Remark</td> </tr> -->
    </tbody>
    </table>
  </div>
</div>

<hr />

<!-- <div class="row" id="dates"> -->
<!--   <div class="col-xs-12"> -->
<!--     <h2>Important Dates</h2> -->
<!--   </div> -->
<!-- </div> -->

<!-- <br> -->
<!-- <div class="row"> -->
<!--   <div class="col-xs-12"> -->
<!--     <table class="table table-striped"> -->
<!--       <tbody> -->
<!--         <tr> -->
<!--           <td>Paper Submission Deadline</td> -->
<!--           <td>to be announced</td> -->
<!--         </tr> -->
<!--         <tr> -->
<!--           <td>Decision Notifications</td> -->
<!--           <td>to be announced</td> -->
<!--         </tr> -->
<!--         <tr> -->
<!--           <td>Camera Ready Paper Deadline</td> -->
<!--           <td>May 15, 2021 (11:59 PM Pacific time)</td> -->
<!--         </tr> -->
<!--         <tr> -->
<!--           <td>Workshop</td> -->
<!--           <td>Between 5th and 8th of September, 2022</td> -->
<!--         </tr> -->
<!--       </tbody> -->
<!--     </table> -->
<!--   </div> -->
<!-- </div> -->

<!-- Speakers -->
<div class="row" id="speakers">
  <div class="col-xs-12">
    <h2>Invited Speakers</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-6 col-lg-3">
    <a href="https://rahmacha.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/rahma-chaabouni.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://rahmacha.github.io/">Rahma Chaabouni</a>
      <h6>DeepMind</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="https://ai.vub.ac.be/team/katie-mudd/">
      <img class="people-pic" src="{{ "/static/img/people/katie-mudd.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://ai.vub.ac.be/team/katie-mudd/">Katie Mudd</a>
      <h6>AI Lab, Vrije Universiteit Brussel</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="https://sites.google.com/site/matspike/">
      <img class="people-pic" src="{{ "/static/img/people/matt-spike.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://sites.google.com/site/matspike/">Matt Spike</a>
      <h6>University of Edinburgh</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="https://douwekiela.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/douwe-kiela.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://douwekiela.github.io/">Douwe Kiela</a>
      <h6>Huggingface</h6>
    </div>
  </div>
</div>

<!-- <hr /> -->

<!-- <div class="row" id="accepted"> -->
<!--   <div class="col-xs-12"> -->
<!--     <h2>Accepted Posters</h2> -->
<!--     <!-1- <p>Note: 3 additional papers were accepted but are not listed here because of an anonymity period.</p> -1-> -->
<!--     <p>to be announced</p> -->
<!--   </div> -->
<!-- </div> -->
<!-- <ul class="paper-list"> -->
<!--     <!-1- <li> -1-> -->
<!--     <!-1-     <span class="paper-title">Do Videos Guide Translations?  Evaluation of a Video-Guided Machine Translation dataset</span><br> -1-> -->
<!--     <!-1-     <span class="paper-authors">Zhishen Yang (Tokyo Institute of Technology); Tosho  Hirasawa (Tokyo Metropolitan University); Naoaki Okazaki (Tokyo Institute of Technology); Mamoru Komachi (Tokyo Metropolitan University)</span><br> -1-> -->
<!--     <!-1-     <span class="paper-meta">[<a href="static/papers-2021/29.pdf">PDF</a>]</span> -1-> -->
<!--     <!-1- </li> -1-> -->
<!-- </ul> -->

<hr />

<!-- Organizers -->
<div class="row" id="organizers">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-6 col-lg-3">
    <a href="https://scholar.google.fr/citations?user=4VwTolgAAAAJ&hl=fr&oi=ao">
      <img class="people-pic" src="{{ "/static/img/people/mathieu-rita.jpeg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://scholar.google.fr/citations?user=4VwTolgAAAAJ&hl=fr&oi=ao">Mathieu Rita</a>
      <h6>INRIA</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="https://lgalke.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/Lukas-Galke.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://lgalke.github.io">Lukas Galke</a>
      <h6>MPI for Psycholinguistics</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="https://fstrub95.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/florianstrub.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://fstrub95.github.io/">Dr. Florian Strub</a>
      <h6>DeepMind</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="http://www.lifl.fr/~pietquin/">
      <img class="people-pic" src="{{ "/static/img/people/olivier-pietquin-dp.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://www.lifl.fr/~pietquin/">Prof. Olivier Pietquin</a>
      <h6>Google Brain</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="http://www.lscp.net/persons/dupoux/">
      <img class="people-pic" src="{{ "/static/img/people/emmanuel-dupoux.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://www.lscp.net/persons/dupoux/">Prof. Emmanuel Dupoux</a>
      <h6>EHESS / Meta AI Research</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="https://www.limorravivevolang.com/">
      <img class="people-pic" src="{{ "/static/img/people/Limor-Raviv.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.limorravivevolang.com/">Dr. Limor Raviv</a>
      <h6>MPI for Psycholinguistics</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="https://ai.vub.ac.be/team/bart-de-boer/">
      <img class="people-pic" src="{{ "/static/img/people/bart-de-boer.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://ai.vub.ac.be/team/bart-de-boer/">Prof. Bart de Boer</a>
      <h6>AI Lab, Vrije Universiteit Brussel</h6>
    </div>
  </div>
</div>

<hr />

<!-- Scientific Committee -->
<!-- <div class="row" id="scientific_committee"> -->
<!--   <div class="col-xs-12"> -->
<!--     <h2>Scientific Committee</h2> -->
<!--   </div> -->
<!-- </div> -->
<!-- <div class="row"> -->
<!--   <div class="col-xs-6 col-lg-3"> -->
<!--     <a href="https://mila.quebec/en/person/aaron-courville/"> -->
<!--       <img class="people-pic" src="{{ "/static/img/people/aaron-courville-dp.jpg" | prepend:site.baseurl }}"> -->
<!--     </a> -->
<!--     <div class="people-name"> -->
<!--       <a href="https://mila.quebec/en/person/aaron-courville/">Aaron Courville</a> -->
<!--       <h6>University of Montreal</h6> -->
<!--     </div> -->
<!--   </div> -->
<!--   <div class="col-xs-6 col-lg-3"> -->
<!--     <a href="http://www.mateuszmalinowski.com/"> -->
<!--       <img class="people-pic" src="{{ "/static/img/people/mateusz-malinowski-dp.jpeg" | prepend:site.baseurl }}"> -->
<!--     </a> -->
<!--     <div class="people-name"> -->
<!--       <a href="http://www.mateuszmalinowski.com/">Mateusz Malinowski</a> -->
<!--       <h6>DeepMind</h6> -->
<!--     </div> -->
<!--   </div> -->
<!--   <div class="col-xs-6 col-lg-3"> -->
<!--     <a href="http://www.lifl.fr/~pietquin/"> -->
<!--       <img class="people-pic" src="{{ "/static/img/people/olivier-pietquin-dp.jpg" | prepend:site.baseurl }}"> -->
<!--     </a> -->
<!--     <div class="people-name"> -->
<!--       <a href="http://www.lifl.fr/~pietquin/">Olivier Pietquin</a> -->
<!--       <h6>Google Brain</h6> -->
<!--     </div> -->
<!--   </div> -->
<!--   <div class="col-xs-6 col-lg-3"> -->
<!--     <a href="http://www-etud.iro.umontreal.ca/~devries/"> -->
<!--       <img class="people-pic" src="/static/img/people/harmdevries.jpg" /> -->
<!--     </a> -->
<!--     <div class="people-name"> -->
<!--       <a href="http://www-etud.iro.umontreal.ca/~devries/">Harm de Vries</a> -->
<!--       <h6>University of Montreal | Element AI</h6> -->
<!--     </div> -->
<!--   </div> -->
<!-- </div> -->

<!-- <hr /> -->

<!-- <div class="row"> -->
<!--   <div class="col-xs-12"> -->
<!--     <h2>Program Committee</h2> -->
<!--   </div> -->
<!-- </div> -->
<!-- <div class="row"> -->
<!--   <div class="col-xs-6"> -->
<!--     <ul> -->
<!--       <li>Abhishek Das (Facebook AI Research)</li> -->
<!--       <li>Adria Recasens (DeepMind)</li> -->
<!--       <li>Alane Suhr (Cornell)</li> -->
<!--       <li>Anna Potapenko (DeepMind)</li> -->
<!--       <li>Arjun Majumdar (Georgia Tech)</li> -->
<!--       <li>Cătălina Cangea (University of Cambridge)</li> -->
<!--       <li>Catherine Wong (Massachusetts Institute of Technology)</li> -->
<!--       <li>Christopher Davis (University of Cambridge)</li> -->
<!--       <li>Daniel Fried (UC Berkeley)</li> -->
<!--       <li>Drew Hudson (Stanford University)</li> -->
<!--       <li>Erik Wijmans (Georgia Tech)</li> -->
<!--       <li>Florian Strub (DeepMind)</li> -->
<!--       <li>Gabriel Ilharco (University of Washington)</li> -->
<!--       <li>Geoffrey Cideron (InstaDeep)</li> -->
<!--       <li>Hammad Ayyubi (Columbia University)</li> -->
<!--       <li>Hao Tan (University of North Carolina Chapel Hill)</li> -->
<!--       <li>Hao Wu (Fudan University)</li> -->
<!--       <li>Haoyue Shi (Toyota Technological Institute at Chicago)</li> -->
<!--       <li>Hedi Ben-younes (Sorbonne université)</li> -->
<!--       <li>Jack Hessel (Allen Institute for AI)</li> -->
<!--       <li>Jacob Krantz (Oregon State University)</li> -->
<!--       <li>Jean-Baptiste Alayrac (DeepMind)</li> -->
<!--     </ul> -->
<!--   </div> -->
<!--   <div class="col-xs-6"> -->
<!--     <ul> -->
<!--       <li>Jiayuan Mao (MIT)</li> -->
<!--       <li>Joel Ye (Georgia Tech)</li> -->
<!--       <li>Johan Ferret (Google Research, Brain Team)</li> -->
<!--       <li>Karan Desai (University of Michigan)</li> -->
<!--       <li>Lisa Anne Hendricks (DeepMind)</li> -->
<!--       <li>Luca Celotti (Université de Sherbrooke)</li> -->
<!--       <li>Mateusz Malinowski (DeepMind)</li> -->
<!--       <li>Mathieu Rita (École polytechnique)</li> -->
<!--       <li>Mathieu Seurin (University of Lille)</li> -->
<!--       <li>Meera Hahn (Georgia Institute of Technology)</li> -->
<!--       <li>Nicholas Tomlin (UC Berkeley)</li> -->
<!--       <li>Olivier Pietquin (2)</li> -->
<!--       <li>Peter Anderson (Google)</li> -->
<!--       <li>Rodolfo Corona (UC Berkeley)</li> -->
<!--       <li>Rowan Zellers (University of Washington)</li> -->
<!--       <li>Ryan Benmalek (Cornell University)</li> -->
<!--       <li>Sanjay Subramanian (Allen Institute for Artificial Intelligence)</li> -->
<!--       <li>Sidd Karamcheti (Stanford University)</li> -->
<!--       <li>Stefan Lee (Oregon State University)</li> -->
<!--       <li>Valts Blukis (Cornell University)</li> -->
<!--       <li>Volkan Cirik (Carnegie Mellon University)</li> -->
<!--     </ul> -->
<!--   </div> -->
<!-- </div> -->

<hr />

<!-- CfP -->
<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Call for Papers</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      The authors are welcome to submit an abstract of up to 300 words on the basis of new, in-progress, or already published work whose content is relevant to the topics of the workshop:
    </p>
    <p>
          <ul>
            <li>Machine learning methods for understanding language evolution</li>
            <li>Emergent communication with reinforcement learning</li>
            <li>Bayesian agent-based modeling for language evolution</li>
          </ul>
      </p>
      <p>Accepted submissions will be presented as a poster in the workshop.</p>
  </div>
</div>

<hr />

<!-- Submission -->
<div class="row" id="guidelines">
  <div class="col-xs-12">
    <h2>Submission Guidelines</h2>
  </div>
</div>
<div class="row">
    <div class="col-xs-12">
    <p><i>Submission details will appear here soon.</i></p>
            <!-- Please upload submissions at: <a style="color:#2980b9;font-weight:400;" href="https://cmt3.research.microsoft.com/VIGIL2021/">cmt3.research.microsoft.com/VIGIL2021</a>. -->
    <ul>
      <!-- <li><b>Previously published work</b>: We welcome previously published papers from non-ML conferences, will also accept cross-submissions from ML conferences (including NAACL 2021) which are within the scope of the workshop without re-formatting. These specific papers do not have to be anonymous. They are eligible for poster sessions and will only have a very light review process.</li> -->
      <!-- <li><b>Unpublished work</b>: All submissions must be in PDF format. The submissions must be formated using the <a style="color:#2980b9;font-weight:400;" href="https://2021.naacl.org/calls/style-and-formatting/">NAACL 2021 LaTeX style file</a>. Submissions are limited to 4 content pages, including all figures and tables; additional pages containing statements of acknowledgements and funding disclosures, and references are allowed. The maximum file size for submissions is 50MB. The CMT-based review process will be double-blind to avoid potential conflicts of interests.</li> -->
    </ul>
    <!-- <p>In case of any issues, feel free to email the workshop organizers at: <a href="mailto:ml4evolangworkshop@lpag.de">ml4evolangworkshop@lpag.de</a>.</p> -->
    </div>
</div>

<hr />
<!-- Intro -->
<!-- <div class="row" id="intro"> -->
<!--     <div class="col-xs-12"> -->
<!--         <h2>Introduction</h2> -->
<!--         <p><i>Coming soon...</i></p> -->
<!--     </div> -->
<!-- </div> -->

<!-- <hr /> -->

<!-- Previous Sessions -->
<!-- <div class="row"> -->
<!--   <div class="col-xs-12"> -->
<!--     <h2>Previous Sessions</h2><a name="/prev_session"></a> -->
<!--   </div> -->
<!-- </div> -->
<!-- <div class="row"> -->
<!--     <div class="col-xs-12"> -->
<!--         <p> -->
<!--             <ul> -->
<!--                 <li><a href="https://nips2017vigil.github.io/">ViGIL Workshop at NeurIPS 2017</a></li> -->
<!--                 <li><a href="https://nips2018vigil.github.io/">ViGIL Workshop at NeurIPS 2018</a></li> -->
<!--                 <li><a href="https://vigilworkshop.github.io/2019">ViGIL Workshop at NeurIPS 2019</a></li> -->
<!--             </ul> -->
<!--         </p> -->
<!--     </div> -->
<!-- </div> -->

<!-- <hr /> -->

<!-- References -->
<!-- <div class="row"> -->
<!--   <div class="col-xs-12"> -->
<!--     <h2>References</h2> -->
<!--   </div> -->
<!-- </div> -->
<!-- <div class="row"> -->
<!--   <div class="col-md-12"> -->
<!--     <ol> -->
<!-- <li>Y. Bisk, A. Holtzman, J. Thomason, J. Andreas, Y. Bengio, J. Chai, M. Lapata, A. Lazaridou, J. May, A. Nisnevich, N. Pinto, -->
<!-- and J. Turian. Experience grounds language. In EMNLP, 2020. </li> -->
<!-- <li>L. Smith and M. Gasser. The development of embodied cognition: Six lessons from babies. Artificial life, 11(1-2):13–29, -->
<!-- 2005. </li> -->
<!-- <li>P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. Sünderhauf, I. Reid, S. Gould, and A. van den Hengel. Vision-andlanguage Navigation: Interpreting Visually-grounded Navigation Instructions in Real Environments. In CVPR, 2018. </li> -->
<!-- <li>A. Suhr, C. Yan, J. Schluger, S. Yu, H. Khader, M. Mouallem, I. Zhang, and Y. Artzi. Executing instructions in situated -->
<!-- collaborative interactions. In EMNLP-IJCNLP, 2019. </li> -->
<!-- <li>D. Misra, A. Bennett, V. Blukis, E. Niklasson, M. Shatkhin, and Y. Artzi. Mapping instructions to actions in 3D environments with visual goal prediction. In EMNLP, 2018. </li> -->
<!-- <li>C. Chen, U. Jain, C. Schissler, S. V. A. Gari, Z. Al-Halah, V. K. Ithapu, P. Robinson, and K. Grauman. SoundSpaces: -->
<!-- Audio-Visual Navigation in 3D Environments. ECCV, 2019. </li> -->
<!-- <li>S. Venugopalan, M. Rohrbach, J. Donahue, R. J. Mooney, T. Darrell, and K. Saenko. Sequence to sequence-video to text. -->
<!-- In ICCV, 2015. </li> -->
<!-- <li>S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, L. C. Zitnick, and D. Parikh. VQA: Visual question answering. In CVPR, -->
<!-- 2015.</li> -->
<!-- <li>A. Das, S. Datta, G. Gkioxari, S. Lee, D. Parikh, and D. Batra. Embodied Question Answering. In CVPR, 2018. </li> -->
<!-- <li>R. Zellers, Y. Bisk, A. Farhadi, and Y. Choi. From recognition to cognition: Visual commonsense reasoning. In CVPR, -->
<!-- 2019. </li> -->
<!-- <li>J. Lei, L. Yu, M. Bansal, and T. L. Berg. TVQA: Localized, compositional video question answering. In EMNLP, 2018. </li> -->
<!-- <li>S. Tellex, N. Gopalan, H. Kress-Gazit, and C. Matuszek. Robots that use language. Annual Review of Control, Robotics, and -->
<!-- Autonomous Systems, 3:25–55, 2020. </li> -->
<!-- <li>V. Blukis, Y. Terme, E. Niklasson, R. A. Knepper, and Y. Artzi. Learning to map natural language instructions to physical -->
<!-- quadcopter control using simulated flight. In CoRL, 2019. </li> -->
<!--     </ol> -->
<!--   </div> -->
<!-- </div> -->

<!-- vim: set ft=html -->
